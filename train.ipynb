{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j1dJOz3Nbq6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "# import resources\n",
        "%matplotlib inline\n",
        "\n",
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow==4.1.1\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from flask import Flask , render_template ,request\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNaA5voD3IY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!mkdir train\n",
        "%cd /content/train\n",
        "#!pwd\n",
        "!unzip -qq /content/train2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNFhkq7xFqqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg16-00b39a1b.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aa7I6dHRLJBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def itot(img, max_size=None):\n",
        "    # Rescale the image\n",
        "    if (max_size==None):\n",
        "        itot_t = transforms.Compose([\n",
        "           \n",
        "            transforms.ToPILImage(),\n",
        "             transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])    \n",
        "    else:\n",
        "        H, W, C = img.shape\n",
        "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
        "        itot_t = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])\n",
        "\n",
        "    # Convert image to tensor\n",
        "    tensor = itot_t(img)\n",
        "\n",
        "    # Add the batch_size dimension\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "    return tensor\n",
        "\n",
        "def load_image(path):\n",
        "    # Images loaded as BGR\n",
        "    img = cv2.imread(path)\n",
        "    return img\n",
        "  \n",
        "def MSELos(input, target):\n",
        "    return torch.sum((input - target)**2) / input.data.nelement()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KhEWjKxMb0oO",
        "colab_type": "code",
        "outputId": "ec5dbb4d-618d-418a-d6f7-e036090c5163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3617
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import vgg\n",
        "import transformer\n",
        "import utils\n",
        "\n",
        "# GLOBAL SETTINGS\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"/content/train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"/content/s5.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 1e-7 #2e-6\n",
        "STYLE_WEIGHT = 65 #50\n",
        "TV_WEIGHT = 1e-6 \n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"/content/\"\n",
        "SAVE_IMAGE_PATH = \"/content/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "\n",
        "def train():\n",
        "    # Seeds\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Device\n",
        "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset and Dataloader\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Load networks\n",
        "    TransformerNetwork = transformer.TransformerNetwork().to(device)\n",
        "    VGG = vgg.VGG16('/content/vgg16-00b39a1b.pth').to(device)\n",
        "\n",
        "    # Get Style Features\n",
        "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
        "    imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
        "    style_image = load_image(STYLE_IMAGE_PATH)\n",
        "    print(type(style_image))\n",
        "    style_tensor = itot(style_image).to(device)\n",
        "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "    B, C, H, W = style_tensor.shape\n",
        "    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "    style_gram = {}\n",
        "    for key, value in style_features.items():\n",
        "        style_gram[key] = utils.gram(value)\n",
        "\n",
        "    # Optimizer settings\n",
        "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "\n",
        "    # Loss trackers\n",
        "    content_loss_history = []\n",
        "    style_loss_history = []\n",
        "    total_loss_history = []\n",
        "    batch_content_loss_sum = 0\n",
        "    batch_style_loss_sum = 0\n",
        "    batch_total_loss_sum = 0\n",
        "\n",
        "    # Optimization/Training Loop\n",
        "    batch_count = 1\n",
        "    start_time = time.time()\n",
        "    for epoch in range (1, NUM_EPOCHS+1):\n",
        "        print(\"========Epoch {}/{}========\".format(epoch, NUM_EPOCHS+1))\n",
        "        for batch_id, (content_batch, _) in enumerate(train_loader):\n",
        "            # Zero-out Gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate images and get features\n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            \n",
        "            \n",
        "            content_loss = MSELos(content_features['relu2_2'], generated_features['relu2_2']).to(device)\n",
        "            content_loss = CONTENT_WEIGHT *  content_loss\n",
        "            batch_content_loss_sum += content_loss\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELos(utils.gram(value), style_gram[key]).to(device)\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "            batch_style_loss_sum += style_loss\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "            batch_total_loss_sum += total_loss.item()\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save Model and Print Losses\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                # Print Losses\n",
        "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
        "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
        "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
        "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
        "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
        "\n",
        "                # Save Model\n",
        "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
        "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
        "                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n",
        "\n",
        "                # Save sample generated image\n",
        "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
        "                sample_image = utils.ttoi(sample_tensor.clone().detach())\n",
        "                sample_image_path = SAVE_IMAGE_PATH + \"sample0_\" + str(batch_count-1) + \".png\"\n",
        "                utils.saveimg(sample_image, sample_image_path)\n",
        "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
        "\n",
        "                # Save loss histories\n",
        "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
        "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "\n",
        "            # Iterate Batch Counter\n",
        "            batch_count+=1\n",
        "\n",
        "    stop_time = time.time()\n",
        "    # Print loss histories\n",
        "    print(\"Done Training the Transformer Network!\")\n",
        "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
        "    print(\"========Content Loss========\")\n",
        "    print(content_loss_history) \n",
        "    print(\"========Style Loss========\")\n",
        "    print(style_loss_history) \n",
        "    print(\"========Total Loss========\")\n",
        "    print(total_loss_history) \n",
        "\n",
        "    # Save TransformerNetwork weights\n",
        "    TransformerNetwork.eval()\n",
        "    TransformerNetwork.cpu()\n",
        "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
        "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
        "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
        "    print(\"Done saving final model\")\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "========Epoch 1/2========\n",
            "========Iteration 1/20696========\n",
            "\tContent Loss:\t0.02\n",
            "\tStyle Loss:\t52166536.00\n",
            "\tTotal Loss:\t52166536.00\n",
            "Time elapsed:\t1.0908713340759277 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_0.pth\n",
            "Saved sample tranformed image at /content/sample0_0.png\n",
            "========Iteration 501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t8162696.50\n",
            "\tTotal Loss:\t8162695.33\n",
            "Time elapsed:\t369.6879234313965 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_500.pth\n",
            "Saved sample tranformed image at /content/sample0_500.png\n",
            "========Iteration 1001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t4345843.50\n",
            "\tTotal Loss:\t4345841.54\n",
            "Time elapsed:\t738.7319476604462 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_1000.pth\n",
            "Saved sample tranformed image at /content/sample0_1000.png\n",
            "========Iteration 1501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t2998235.25\n",
            "\tTotal Loss:\t2998232.47\n",
            "Time elapsed:\t1107.727221250534 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_1500.pth\n",
            "Saved sample tranformed image at /content/sample0_1500.png\n",
            "========Iteration 2001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t2305438.75\n",
            "\tTotal Loss:\t2305438.63\n",
            "Time elapsed:\t1476.654625415802 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_2000.pth\n",
            "Saved sample tranformed image at /content/sample0_2000.png\n",
            "========Iteration 2501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1882363.00\n",
            "\tTotal Loss:\t1882360.26\n",
            "Time elapsed:\t1845.3987967967987 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_2500.pth\n",
            "Saved sample tranformed image at /content/sample0_2500.png\n",
            "========Iteration 3001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1596149.75\n",
            "\tTotal Loss:\t1596150.47\n",
            "Time elapsed:\t2213.9169583320618 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_3000.pth\n",
            "Saved sample tranformed image at /content/sample0_3000.png\n",
            "========Iteration 3501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1388626.50\n",
            "\tTotal Loss:\t1388628.18\n",
            "Time elapsed:\t2582.488267660141 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_3500.pth\n",
            "Saved sample tranformed image at /content/sample0_3500.png\n",
            "========Iteration 4001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1231646.75\n",
            "\tTotal Loss:\t1231648.63\n",
            "Time elapsed:\t2950.9657945632935 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_4000.pth\n",
            "Saved sample tranformed image at /content/sample0_4000.png\n",
            "========Iteration 4501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1108104.12\n",
            "\tTotal Loss:\t1108107.26\n",
            "Time elapsed:\t3318.9799077510834 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_4500.pth\n",
            "Saved sample tranformed image at /content/sample0_4500.png\n",
            "========Iteration 5001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t1008958.31\n",
            "\tTotal Loss:\t1008961.10\n",
            "Time elapsed:\t3686.1223826408386 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_5000.pth\n",
            "Saved sample tranformed image at /content/sample0_5000.png\n",
            "========Iteration 5501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t926622.94\n",
            "\tTotal Loss:\t926626.13\n",
            "Time elapsed:\t4052.776529788971 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_5500.pth\n",
            "Saved sample tranformed image at /content/sample0_5500.png\n",
            "========Iteration 6001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t857761.94\n",
            "\tTotal Loss:\t857764.54\n",
            "Time elapsed:\t4419.258016347885 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_6000.pth\n",
            "Saved sample tranformed image at /content/sample0_6000.png\n",
            "========Iteration 6501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t799013.00\n",
            "\tTotal Loss:\t799015.57\n",
            "Time elapsed:\t4785.751591205597 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_6500.pth\n",
            "Saved sample tranformed image at /content/sample0_6500.png\n",
            "========Iteration 7001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t748385.56\n",
            "\tTotal Loss:\t748387.15\n",
            "Time elapsed:\t5152.298470020294 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_7000.pth\n",
            "Saved sample tranformed image at /content/sample0_7000.png\n",
            "========Iteration 7501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t704298.12\n",
            "\tTotal Loss:\t704299.46\n",
            "Time elapsed:\t5518.761332511902 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_7500.pth\n",
            "Saved sample tranformed image at /content/sample0_7500.png\n",
            "========Iteration 8001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t665454.25\n",
            "\tTotal Loss:\t665454.58\n",
            "Time elapsed:\t5884.947822332382 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_8000.pth\n",
            "Saved sample tranformed image at /content/sample0_8000.png\n",
            "========Iteration 8501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t631129.81\n",
            "\tTotal Loss:\t631129.91\n",
            "Time elapsed:\t6251.169906377792 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_8500.pth\n",
            "Saved sample tranformed image at /content/sample0_8500.png\n",
            "========Iteration 9001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t600385.25\n",
            "\tTotal Loss:\t600385.28\n",
            "Time elapsed:\t6617.416640043259 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_9000.pth\n",
            "Saved sample tranformed image at /content/sample0_9000.png\n",
            "========Iteration 9501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t572951.88\n",
            "\tTotal Loss:\t572951.53\n",
            "Time elapsed:\t6983.77849817276 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_9500.pth\n",
            "Saved sample tranformed image at /content/sample0_9500.png\n",
            "========Iteration 10001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t548031.62\n",
            "\tTotal Loss:\t548031.53\n",
            "Time elapsed:\t7350.0452971458435 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_10000.pth\n",
            "Saved sample tranformed image at /content/sample0_10000.png\n",
            "========Iteration 10501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t525312.75\n",
            "\tTotal Loss:\t525312.85\n",
            "Time elapsed:\t7716.424051761627 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_10500.pth\n",
            "Saved sample tranformed image at /content/sample0_10500.png\n",
            "========Iteration 11001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t504613.75\n",
            "\tTotal Loss:\t504613.87\n",
            "Time elapsed:\t8082.748618841171 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_11000.pth\n",
            "Saved sample tranformed image at /content/sample0_11000.png\n",
            "========Iteration 11501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t485621.47\n",
            "\tTotal Loss:\t485621.07\n",
            "Time elapsed:\t8449.011343955994 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_11500.pth\n",
            "Saved sample tranformed image at /content/sample0_11500.png\n",
            "========Iteration 12001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t468170.06\n",
            "\tTotal Loss:\t468169.85\n",
            "Time elapsed:\t8815.247530460358 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_12000.pth\n",
            "Saved sample tranformed image at /content/sample0_12000.png\n",
            "========Iteration 12501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t452094.12\n",
            "\tTotal Loss:\t452094.21\n",
            "Time elapsed:\t9181.423747301102 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_12500.pth\n",
            "Saved sample tranformed image at /content/sample0_12500.png\n",
            "========Iteration 13001/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t437155.38\n",
            "\tTotal Loss:\t437155.22\n",
            "Time elapsed:\t9547.527351617813 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_13000.pth\n",
            "Saved sample tranformed image at /content/sample0_13000.png\n",
            "========Iteration 13501/20696========\n",
            "\tContent Loss:\t0.03\n",
            "\tStyle Loss:\t423266.09\n",
            "\tTotal Loss:\t423266.07\n",
            "Time elapsed:\t9913.562498569489 seconds\n",
            "Saved TransformerNetwork checkpoint file at /content/checkpoint_13500.pth\n",
            "Saved sample tranformed image at /content/sample0_13500.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kekf55CrdZla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}