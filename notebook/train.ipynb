{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train( fast neural network ).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j1dJOz3Nbq6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "# import resources\n",
        "%matplotlib inline\n",
        "\n",
        "!pip uninstall -y Pillow\n",
        "# install the new one\n",
        "!pip install Pillow==4.1.1\n",
        "# import the new one\n",
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from flask import Flask , render_template ,request\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNaA5voD3IY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!mkdir train\n",
        "%cd /content/train\n",
        "#!pwd\n",
        "!unzip -qq /content/train2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KNFhkq7xFqqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg16-00b39a1b.pth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aa7I6dHRLJBe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def itot(img, max_size=None):\n",
        "    # Rescale the image\n",
        "    if (max_size==None):\n",
        "        itot_t = transforms.Compose([\n",
        "           \n",
        "            transforms.ToPILImage(),\n",
        "             transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])    \n",
        "    else:\n",
        "        H, W, C = img.shape\n",
        "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
        "        itot_t = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda x: x.mul(255))\n",
        "        ])\n",
        "\n",
        "    # Convert image to tensor\n",
        "    tensor = itot_t(img)\n",
        "\n",
        "    # Add the batch_size dimension\n",
        "    tensor = tensor.unsqueeze(dim=0)\n",
        "    return tensor\n",
        "\n",
        "def load_image(path):\n",
        "    # Images loaded as BGR\n",
        "    img = cv2.imread(path)\n",
        "    return img\n",
        "  \n",
        "def MSELos(input, target):\n",
        "    return torch.sum((input - target)**2) / input.data.nelement()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KhEWjKxMb0oO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import vgg\n",
        "import transformer\n",
        "import utils\n",
        "\n",
        "# GLOBAL SETTINGS\n",
        "TRAIN_IMAGE_SIZE = 256\n",
        "DATASET_PATH = \"/content/train\"\n",
        "NUM_EPOCHS = 1\n",
        "STYLE_IMAGE_PATH = \"/content/s5.jpg\"\n",
        "BATCH_SIZE = 4 \n",
        "CONTENT_WEIGHT = 1e-7 #2e-6\n",
        "STYLE_WEIGHT = 65 #50\n",
        "TV_WEIGHT = 1e-6 \n",
        "ADAM_LR = 0.001\n",
        "SAVE_MODEL_PATH = \"/content/\"\n",
        "SAVE_IMAGE_PATH = \"/content/\"\n",
        "SAVE_MODEL_EVERY = 500 # 2,000 Images with batch size 4\n",
        "SEED = 35\n",
        "\n",
        "def train():\n",
        "    # Seeds\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    random.seed(SEED)\n",
        "\n",
        "    # Device\n",
        "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dataset and Dataloader\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
        "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Load networks\n",
        "    TransformerNetwork = transformer.TransformerNetwork().to(device)\n",
        "    VGG = vgg.VGG16('/content/vgg16-00b39a1b.pth').to(device)\n",
        "\n",
        "    # Get Style Features\n",
        "    imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
        "    imagenet_mean = torch.tensor([103.939, 116.779, 123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
        "    style_image = load_image(STYLE_IMAGE_PATH)\n",
        "    print(type(style_image))\n",
        "    style_tensor = itot(style_image).to(device)\n",
        "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
        "    B, C, H, W = style_tensor.shape\n",
        "    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
        "    style_gram = {}\n",
        "    for key, value in style_features.items():\n",
        "        style_gram[key] = utils.gram(value)\n",
        "\n",
        "    # Optimizer settings\n",
        "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
        "\n",
        "    # Loss trackers\n",
        "    content_loss_history = []\n",
        "    style_loss_history = []\n",
        "    total_loss_history = []\n",
        "    batch_content_loss_sum = 0\n",
        "    batch_style_loss_sum = 0\n",
        "    batch_total_loss_sum = 0\n",
        "\n",
        "    # Optimization/Training Loop\n",
        "    batch_count = 1\n",
        "    start_time = time.time()\n",
        "    for epoch in range (1, NUM_EPOCHS+1):\n",
        "        print(\"========Epoch {}/{}========\".format(epoch, NUM_EPOCHS+1))\n",
        "        for batch_id, (content_batch, _) in enumerate(train_loader):\n",
        "            # Zero-out Gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate images and get features\n",
        "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
        "            generated_batch = TransformerNetwork(content_batch)\n",
        "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
        "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
        "\n",
        "            # Content Loss\n",
        "            \n",
        "            \n",
        "            content_loss = MSELos(content_features['relu2_2'], generated_features['relu2_2']).to(device)\n",
        "            content_loss = CONTENT_WEIGHT *  content_loss\n",
        "            batch_content_loss_sum += content_loss\n",
        "\n",
        "            # Style Loss\n",
        "            style_loss = 0\n",
        "            for key, value in generated_features.items():\n",
        "                s_loss = MSELos(utils.gram(value), style_gram[key]).to(device)\n",
        "                style_loss += s_loss\n",
        "            style_loss *= STYLE_WEIGHT\n",
        "            batch_style_loss_sum += style_loss\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = content_loss + style_loss\n",
        "            batch_total_loss_sum += total_loss.item()\n",
        "\n",
        "            # Backprop and Weight Update\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Save Model and Print Losses\n",
        "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
        "                # Print Losses\n",
        "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
        "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
        "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
        "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
        "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
        "\n",
        "                # Save Model\n",
        "                checkpoint_path = SAVE_MODEL_PATH + \"checkpoint_\" + str(batch_count-1) + \".pth\"\n",
        "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
        "                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n",
        "\n",
        "                # Save sample generated image\n",
        "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
        "                sample_image = utils.ttoi(sample_tensor.clone().detach())\n",
        "                sample_image_path = SAVE_IMAGE_PATH + \"sample0_\" + str(batch_count-1) + \".png\"\n",
        "                utils.saveimg(sample_image, sample_image_path)\n",
        "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
        "\n",
        "                # Save loss histories\n",
        "                content_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
        "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
        "\n",
        "            # Iterate Batch Counter\n",
        "            batch_count+=1\n",
        "\n",
        "    stop_time = time.time()\n",
        "    # Print loss histories\n",
        "    print(\"Done Training the Transformer Network!\")\n",
        "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
        "    print(\"========Content Loss========\")\n",
        "    print(content_loss_history) \n",
        "    print(\"========Style Loss========\")\n",
        "    print(style_loss_history) \n",
        "    print(\"========Total Loss========\")\n",
        "    print(total_loss_history) \n",
        "\n",
        "    # Save TransformerNetwork weights\n",
        "    TransformerNetwork.eval()\n",
        "    TransformerNetwork.cpu()\n",
        "    final_path = SAVE_MODEL_PATH + \"transformer_weight.pth\"\n",
        "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
        "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
        "    print(\"Done saving final model\")\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kekf55CrdZla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}